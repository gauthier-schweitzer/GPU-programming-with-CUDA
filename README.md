# To the attention of Pr. Lokman ABBAS TURKI

This file aims at helping you understand the structure of our project.

## The report `report.pdf`
We wrote a small report that sums up the steps used in Gragg's algorithm, presents the choice we have made to implement it and highlights the main results with a few chosen graphs.



## The different codes
We wrote different programs, corresponding to different tasks that we wanted them to perform.

### Testing the different algorithms/ technologies individually, in `Code/Individual_Algo/`
`main_cpu_double.cu`: *CPU is used, with a double precision* to compute the n roots of a secular equation, n being given by the user. </br></br>
`main_cpu_float.cu`: *CPU is used, with a single precision* to compute the n roots of a secular equation, n being given by the user. </br></br>
`main_gpu_double.cu`: *GPU is used, with a single precision* to compute the n roots of a secular equation, n being given by the user. **This code is the one that has the highest amount of commentaries to illustrate the details of our approach in CUDA**.

### Comparing the performance, in `Code/Comparison/`

#### On the terminal, in `Code/Comparison/Console output`
`comp_console.cu`: *Both CPU and GPU with single precision* are used to compute the n roots of a secular equation, n being given by the user. The differential in performance can be seen immediately in the console (running time and magnitude of the loss). *The GPU procedure used in this setting does not use shared memory and the GPU is not warmed up before*. </br>

#### Through a generated csv, in `Code/Comparison/Table output`
These codes were mainly used to generate the graphs are compare the performance of the different procedures in an efficient and visual way.</br></br>
`comp_table.cu`: *Both CPU and GPU with single precision* are used to compute the n roots of a secular equation. A range of n is given by the user and performance (running time and magnitude of the error) is stored on a csv file ('result.csv'). To compare them on a fair basis, the user can choose to run the test several time. For each n being tested, the GPU is warmed-up before the first iteration. To try with high values of n, the user can also choose not to compute the roots with the CPU (only GPU). The GPU procedure used in this setting does not use shared memory. </br></br>
`double.cu`: Performs the same task with *double precision for the CPU* (output is 'result_double.csv')</br></br>
`memory.cu`: Performs the same task with the GPU, in a *version using shared memory* (output is 'result_mem.csv')</br></br>
`initialization.cu`: Performs the same task with CPU, the algorithm being *initialized randomly* (output is 'result_init.csv')

## The data, in `Notebooks/csv_files/`
All of the csv files generated by previous algorithms are stored there. This folder is useful for the following Python Notebook.

## The Python Notebooks, in `Notebooks/`
`Graphs.ipynb`: used to exploit the different csv and produce the graphs that are in the report.
`PythonCheckForLi`: used to code the CPU algorithm in Python and check whether we obtained the same roots or not.
